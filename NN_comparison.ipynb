{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nJupyter Notebook: Comparing Standard vs Optimized Neural Networks\\n=================================================================\\n\\nThis notebook compares the training performance, accuracy, and computational efficiency of a standard \\nNumPy-based neural network vs an optimized version using the MNIST dataset.\\n\\nMetrics analyzed:\\n- Training speed (time per epoch)\\n- Accuracy on test data\\n- Memory usage (float64 vs float32 impact)\\n\\nAuthor: Abdullah\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jupyter Notebook: Comparing Standard vs Optimized Neural Networks\n",
    "=================================================================\n",
    "\n",
    "This notebook compares the training performance, accuracy, and computational efficiency of a standard \n",
    "NumPy-based neural network vs an optimized version using the MNIST dataset.\n",
    "\n",
    "Metrics analyzed:\n",
    "- Training speed (time per epoch)\n",
    "- Accuracy on test data\n",
    "- Memory usage (float64 vs float32 impact)\n",
    "\n",
    "Author: Abdullah\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from NeuralNetwork import NeuralNetwork as StandardNN \n",
    "from optimized_NeuralNetwork import NeuralNetwork as OptimizedNN  \n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  56000\n",
      "Test:  14000\n",
      "Epoch 0: Loss = 0.0020\n",
      "Epoch 1: Loss = 0.0006\n",
      "Epoch 2: Loss = 0.0008\n",
      "Epoch 3: Loss = 0.0007\n",
      "Epoch 4: Loss = 0.0003\n",
      "Epoch 0: Loss = 0.0396\n",
      "Epoch 1: Loss = 0.0235\n",
      "Epoch 2: Loss = 0.0000\n",
      "Epoch 3: Loss = 0.0105\n",
      "Epoch 4: Loss = 0.0059\n",
      "Metrics: \n",
      "Eval: Acc Std: 0.97, Acc Opt: 0.95\n",
      "Time: \n",
      "Standard NN Training Time: 39.21 sec\n",
      "Optimized NN Training Time: 8.75 sec\n",
      "Memory Usage: \n",
      "Memory Usage Standard NN: 855.33 KB\n",
      "Memory Usage Optimized NN: 854.54 KB\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "X = mnist.data.to_numpy() / 255.0  # Normalize pixel values\n",
    "Y = mnist.target.astype(int).to_numpy().reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding for labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "Y = encoder.fit_transform(Y)\n",
    "\n",
    "# data Split \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train: \", len(X_train))\n",
    "print(\"Test: \", len(X_test))\n",
    "\n",
    "layer_sizes = [784, 128, 64, 10]\n",
    "activations = [\"relu\", \"relu\", \"softmax\"]\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "\n",
    "# Initialize Networks\n",
    "nn_standard = StandardNN(layer_sizes, activations, learning_rate)\n",
    "nn_optimized = OptimizedNN(layer_sizes, activations, learning_rate)\n",
    "\n",
    "\n",
    "start_time_std = time.time()\n",
    "nn_standard.train(X_train, Y_train, epochs=epochs)\n",
    "std_time = time.time() - start_time_std\n",
    "\n",
    "start_time_opt = time.time()\n",
    "nn_optimized.train(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "opt_time = time.time() - start_time_opt\n",
    "\n",
    "\n",
    "\n",
    "accuracy_std = []\n",
    "accuracy_opt = []\n",
    "\n",
    "pred_std = []\n",
    "pred_opt = []\n",
    "\n",
    "for x in X_test: \n",
    "    pred_std.append(np.argmax(nn_standard.predict(x)))  \n",
    "    pred_opt.append(np.argmax(nn_optimized.predict(x)))  \n",
    "\n",
    "pred_std = np.array(pred_std) \n",
    "pred_opt = np.array(pred_opt)\n",
    "\n",
    "acc_std = np.mean(pred_std == np.argmax(Y_test, axis=1))\n",
    "acc_opt = np.mean(pred_opt == np.argmax(Y_test, axis=1))\n",
    "\n",
    "accuracy_std.append(acc_std)\n",
    "accuracy_opt.append(acc_opt)\n",
    "\n",
    "print(\"Metrics: \")\n",
    "print(f\"Eval: Acc Std: {np.mean(accuracy_opt):.2f}, Acc Opt: {np.mean(accuracy_std):.2f}\")\n",
    "\n",
    "print(\"Time: \")\n",
    "print(f\"Standard NN Training Time: {std_time:.2f} sec\")\n",
    "print(f\"Optimized NN Training Time: {opt_time:.2f} sec\")\n",
    "\n",
    "print(\"Memory Usage: \")\n",
    "std_mem = sum(sys.getsizeof(param) for param in nn_standard.weights + nn_standard.biases)\n",
    "opt_mem = sum(sys.getsizeof(param) for param in nn_optimized.weights + nn_optimized.biases)\n",
    "print(f\"Memory Usage Standard NN: {std_mem / 1024:.2f} KB\")\n",
    "print(f\"Memory Usage Optimized NN: {opt_mem / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
